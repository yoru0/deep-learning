{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b82c0ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90b3e946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>de</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Donnerwetter!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Help!</td>\n",
       "      <td>Hilfe!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop!</td>\n",
       "      <td>Stopp!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      en             de\n",
       "0    Go.           Geh.\n",
       "1   Run!          Lauf!\n",
       "2   Wow!  Donnerwetter!\n",
       "3  Help!         Hilfe!\n",
       "4  Stop!         Stopp!"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"dictionary.csv\", encoding='utf-8')\n",
    "dataset = dataset.dropna()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31b9e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 5000\n",
    "MAX_SEQUENCE_LENGTH = 32\n",
    "UNITS = 256 \n",
    "EMBEDDING_DIM = 256\n",
    "BUFFER = 5000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "english_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize = \"lower_and_strip_punctuation\",\n",
    "    max_tokens = MAX_SEQUENCE_LENGTH, \n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = MAX_SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "german_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize = \"lower_and_strip_punctuation\",\n",
    "    max_tokens = MAX_SEQUENCE_LENGTH, \n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = MAX_SEQUENCE_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e024157f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go.', 'run!', 'wow!']\n",
      "['starttoken geh. endtoken', 'starttoken lauf! endtoken', 'starttoken donnerwetter! endtoken']\n"
     ]
    }
   ],
   "source": [
    "def standardize_text(text, is_target):\n",
    "    w = text.lower()\n",
    "    w = re.sub(r'[\" \"]+', \" \",  w)\n",
    "    w = w.strip()\n",
    "\n",
    "    if is_target:\n",
    "        w = 'starttoken ' + w + ' endtoken'\n",
    "    return w\n",
    "\n",
    "dataset[\"eng\"] = dataset[\"en\"].apply(lambda x: standardize_text(x, is_target=False))\n",
    "dataset[\"ger\"] = dataset[\"de\"].apply(lambda x: standardize_text(x, is_target=True))\n",
    "\n",
    "english_lang = dataset[\"eng\"].tolist()\n",
    "german_lang = dataset[\"ger\"].tolist()\n",
    "\n",
    "print(english_lang[:3])\n",
    "print(german_lang[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b32fb896",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train, english_val, german_train, german_val = train_test_split(english_lang, german_lang, test_size=0.2, random_state=42)\n",
    "\n",
    "english_vectorizer.adapt(english_train)\n",
    "german_vectorizer.adapt(german_train)\n",
    "\n",
    "input_vector = english_vectorizer(english_train)\n",
    "output_vector = english_vectorizer(german_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acaf7f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '', 1: '[UNK]', 2: 'tom', 3: 'you', 4: 'i', 5: 'to', 6: 'the', 7: 'a', 8: 'is', 9: 'do', 10: 'that', 11: 'it', 12: 'he', 13: 'me', 14: 'this', 15: 'have', 16: 'was', 17: 'in', 18: 'dont', 19: 'my', 20: 'are', 21: 'of', 22: 'im', 23: 'what', 24: 'your', 25: 'we', 26: 'for', 27: 'like', 28: 'be', 29: 'know'}\n",
      "{0: '', 1: '[UNK]', 2: 'starttoken', 3: 'endtoken', 4: 'ich', 5: 'tom', 6: 'ist', 7: 'nicht', 8: 'das', 9: 'du', 10: 'sie', 11: 'es', 12: 'zu', 13: 'hat', 14: 'die', 15: 'er', 16: 'wir', 17: 'ein', 18: 'habe', 19: 'der', 20: 'was', 21: 'mir', 22: 'in', 23: 'mich', 24: 'ihr', 25: 'dass', 26: 'war', 27: 'wie', 28: 'eine', 29: 'sich'}\n"
     ]
    }
   ],
   "source": [
    "input_vocab = {index: word for index, word in enumerate(english_vectorizer.get_vocabulary())}\n",
    "output_vocab = {index: word for index, word in enumerate(german_vectorizer.get_vocabulary())}\n",
    "\n",
    "print(dict(list(input_vocab.items())[:30]))\n",
    "print(dict(list(output_vocab.items())[:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9c5c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(input_text, target_text):\n",
    "    encoder_input = english_vectorizer(input_text)\n",
    "\n",
    "    decoder_input_text = [text.replace(\" endtoken\",  \"\") for text in target_text]\n",
    "    decoder_output_text = [text.replace(\"starttoken \", \"\") for text in target_text]\n",
    "\n",
    "    decoder_input = german_vectorizer(decoder_input_text)\n",
    "    decoder_output = german_vectorizer(decoder_output_text)\n",
    "\n",
    "    return {\"input_1\": encoder_input, \"input_2\": decoder_input}, decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25deca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = vectorize(english_train, german_train)\n",
    "x_val, y_val = vectorize(english_val, german_val)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(BUFFER).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bb2eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocabulary_size, embedding_dim, units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocabulary_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.LSTM(units, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, state_h, state_c = self.lstm(x)\n",
    "        return output, state_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697cd638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
